# n4m-handpose
Wraps [MediaPipe Handpose](https://github.com/tensorflow/tfjs-models/tree/master/handpose) inside electron and serves the detected parts via MaxAPI.

![handpose](https://user-images.githubusercontent.com/43569216/81646758-261df680-946f-11ea-825a-81d8d81abe4e.gif)

Based around [Yuichi Yogo](https://github.com/yuichkun)'s great work porting Electron + Tensorflow models into [Node For Max](https://github.com/Cycling74/n4m-examples)

- Handpose is being used in Arthur Parmentier'sÂ [Soundpainting](https://github.com/arthur-parmentier/soundpainting-signs-gestures-recognition) Project
- *Are you using n4m-handpose in your project? Let me know*


# Steps
1. Install npm dependencies by clicking the indicated button. Since Electron's kind of big in size, this make take a while depending on your network environment. When the message object says it's 'completed' you'll never have to do this step again.
2. Click on the toggle button to launch MediaPipe Handpose. 
3. The node.script emits the detected results as a dictionary, which has been parsed into (x,y,z) lists of each finger's tip positions & palmbase
4. All landmarks are also aggregated to a list to send to [Wekinator](http://www.wekinator.org/) if you'd like to perform gesture recognition with the data (sending to port 11000)


### Changelog
- n4m-handposeM4L (Max For Live Device) AMXD now available
- Handpose model controls now available in electron window UI
- Handpose model control settings save automatically and persist/reload 
- Render window on or offscreen using arguments to node.script
- Hand landmarks are scaled and rendered to a jitter window (m4l + maxmsp)
